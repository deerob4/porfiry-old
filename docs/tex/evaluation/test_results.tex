\section{Evaluation of Test Results} % (fold)
\label{sec:evaluation_of_test_results}
Throughout development of the system, an extensive amount of testing was performed, both formally, using the created test plan; and informally, by continually recompiling the program to check that areas like the layout of the system were functioning correctly. The development environment with which the system was created aided very much in this: every time a change was made to the code, be it a small change like modifying the colour of an input box; or a larger modification, the system automatically recompiled an updated, without having to restart it; changes appeared instantly. This environment increased the ease with which the functionality of the system could be tested. This came in particularly useful when developing the colour scheme algorithm. If there was a long wait time after generating a potential new scheme, it would have taken much more time to iterate on colours and eventually find an effective scheme. A simpler example was when events handlers were being attached to buttons. Though a fairly simple task, it is easy to make a mistake, such as including the function directly rather than wrapping it in a closure. The environment that any issues were quickly identified and fixed, saving a lot of time. This system of testing also meant that, in the event of a test not passing, a new solution could be quickly devised and deployed to the system, then retested, without having to wait for it to compile again. Of course, there were some issues with this approach: the system is built almost entirely from small modules, composed together into a larger whole. This meant that if a module could not be found (it may have been renamed, or moved to a different location), the compilation would fail, and the problem would have to be fixed; this was usually very easy, however.

Another tool that sped up the testing of the system was a syntax linter in the code editor. This meant that any syntax errors that might result in a failed compilation, and therefore slow down the testing, and potentially reduce its accuracy, were made visible during the actual editor, before compilation was even performed. This came in particularly useful when importing other modules; if a module was not available, the linter notified this, so the problem could quickly be fixed.

Devising a test plan first, with normal, extreme and erroneous data; and then using it as a guide for how to test the system, proved an effective strategy. It meant that all potential cases have been covered and accounted for, and the school are far less likely run into any issues with the system. It also allowed for improvements in the interface to be discovered: in the question and break length inputs, there was originally an error message upon entering a non-numerical character. Testing revealed that this error message interfered with the layout of the system's interface, and a more effective solution, both in terms of user-experience and aesthetics, would be to have the system simply not accept non-numerical characters.
% section evaluation_of_test_results (end)
